{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "from nltk.probability import FreqDist\n",
      "from nltk.stem.wordnet import WordNetLemmatizer\n",
      "import csv\n",
      "import os\n",
      "\n",
      "# global vars:\n",
      "all_unigram = 'all_unigram.csv'\n",
      "all_bigram = 'all_bigram.csv'\n",
      "all_trigram = 'all_trigram.csv'\n",
      "\n",
      "\n",
      "# function to clean the data and create a clean output word file\n",
      "#            - stopwords\n",
      "#            - stemming\n",
      "#            - punctunations \n",
      "def cleanFile(filename):\n",
      "    \n",
      "    inputFile = open(filename, \"r\")\n",
      "    stops = set(stopwords.words('english'))\n",
      "    #filtered_words = []\n",
      "    stems = WordNetLemmatizer()\n",
      "    tokens = RegexpTokenizer(r'\\w+')\n",
      "    \n",
      "    cleanfile = os.path.splitext(filename)[0] + '_clean.csv'    \n",
      "    cleanfp = csv.writer(open(cleanfile,'wb'))\n",
      "            \n",
      "    for line in inputFile:\n",
      "        line = tokens.tokenize(line)\n",
      "        #print line;\n",
      "        for w in line:        \n",
      "            if w.lower() not in stops:\n",
      "                if len(w) > 2:\n",
      "                    filtered_words.append(stems.lemmatize(w))\n",
      "                    cleanfp.writerow([w])\n",
      "                    #print lmtzr.lemmatize(w)\n",
      "                \n",
      "    inputFile.close() \n",
      "    return cleanfile\n",
      "\n",
      "    \n",
      "# function to create n-grams files\n",
      "#            - unigrams\n",
      "#            - bigrams\n",
      "#            - trigrams \n",
      "\n",
      "def ngramFile(filename):\n",
      "    ### open input file in read mode and output files in \n",
      "    ### write mode\n",
      "    \n",
      "    cleanFile = csv.reader(open(filename, \"rb\"))   \n",
      "    filename = os.path.splitext(filename)[0]\n",
      "    filename = filename.split('_')[0]\n",
      "    #print filename\n",
      "    \n",
      "    # create unigram file\n",
      "    out_unigram = filename+'_unigram.csv'    \n",
      "    fp_unigram = csv.writer(open(out_unigram,'wb'))\n",
      "    \n",
      "    ### create bigram file\n",
      "    out_bigram = filename+'_bigram.csv'    \n",
      "    fp_bigram = csv.writer(open(out_bigram,'wb'))\n",
      "    \n",
      "    ### create trigram file\n",
      "    out_trigram = filename+'_trigram.csv'    \n",
      "    fp_trigram = csv.writer(open(out_trigram,'wb'))\n",
      "    \n",
      "    ### create n-gram files containing only words[not freq] from all documents\n",
      "    fp_all_unigram = csv.writer(open(all_unigram,'a'))\n",
      "    fp_all_bigram = csv.writer(open(all_bigram,'a'))\n",
      "    fp_all_trigram = csv.writer(open(all_trigram,'a'))\n",
      "    \n",
      "    ### extract words and create a list\n",
      "    wordList = []\n",
      "    for word in cleanFile:\n",
      "        wordList.append(word[0])\n",
      "        fp_all_unigram.writerow([word[0]])\n",
      "    \n",
      "    ### Create unigram freq count\n",
      "    unigramWordList = FreqDist(wordList)\n",
      "    for k,v in unigramWordList.items():\n",
      "        #print k,v\n",
      "        fp_unigram.writerow([k] + [v])\n",
      "        \n",
      "    ### Create bigrams freq count\n",
      "    bigramWordList = nltk.bigrams(wordList)\n",
      "    fbigramWordList = nltk.FreqDist(bigramWordList)\n",
      "    for k,v in fbigramWordList.items():\n",
      "        #print k,v\n",
      "        fp_bigram.writerow([k] + [v])\n",
      "        \n",
      "        \n",
      "    ### Create trigrams freq count\n",
      "    trigramWordList = nltk.trigrams(wordList)\n",
      "    trigramWordList = nltk.FreqDist(trigramWordList)\n",
      "    for k,v in trigramWordList.items():\n",
      "        #print k,v\n",
      "        fp_trigram.writerow([k] + [v])\n",
      "        \n",
      "    ### adding data to all bigrams and trigrams\n",
      "    bigramWordList = nltk.bigrams(wordList) #### TODO: weird that i need to add this again to fill the bigram list else its empty.\n",
      "    for i in bigramWordList:\n",
      "        fp_all_bigram.writerow([i])\n",
      "    for i in trigramWordList:\n",
      "        fp_all_trigram.writerow([i])\n",
      "  \n",
      "    \n",
      "def calcFreq(filename):\n",
      "    fp_infile = csv.reader(open(filename, \"rb\"))   \n",
      "    filename = os.path.splitext(filename)[0]\n",
      "    filename = filename + '_freq.csv'\n",
      "    #print filename\n",
      "    \n",
      "    fp_outfile = csv.writer(open(filename,'wb'))\n",
      "    \n",
      "    wordList = []\n",
      "    for word in fp_infile:\n",
      "        wordList.append(word[0])\n",
      "    \n",
      "    ####Create your unigram\n",
      "    freqCountWordList = FreqDist(wordList)\n",
      "    \n",
      "    ##### put it file\n",
      "    for k,v in freqCountWordList.items():\n",
      "        #print k,v\n",
      "        fp_outfile.writerow([k] + [v])\n",
      "\n",
      "\n",
      "######### driver function:\n",
      "\n",
      "#### create files\n",
      "ngramFile(cleanFile(\"6334220.txt\"))\n",
      "ngramFile(cleanFile(\"6334221.txt\"))\n",
      "ngramFile(cleanFile(\"6334222.txt\"))\n",
      "ngramFile(cleanFile(\"6334223.txt\"))\n",
      "ngramFile(cleanFile(\"6334224.txt\"))\n",
      "ngramFile(cleanFile(\"6334225.txt\"))\n",
      "ngramFile(cleanFile(\"6334226.txt\"))\n",
      "ngramFile(cleanFile(\"6334227.txt\"))\n",
      "ngramFile(cleanFile(\"6334228.txt\"))\n",
      "ngramFile(cleanFile(\"6334229.txt\"))\n",
      "\n",
      "#### summary files\n",
      "calcFreq('all_bigram.csv')\n",
      "calcFreq('all_unigram.csv')\n",
      "calcFreq('all_trigram.csv')\n",
      "\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [],
     "prompt_number": 201
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 198
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}